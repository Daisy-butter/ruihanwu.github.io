<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ruihanwu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruihanwu.github.io/" rel="alternate" type="text/html" /><updated>2025-10-26T07:03:38+00:00</updated><id>https://ruihanwu.github.io/feed.xml</id><title type="html">Ruihan’s Log</title><subtitle>Notes on Embodied Intelligence, World Models, and Robotics.</subtitle><author><name>Ruihan Wu</name><email>22307140084@m.fudan.edu.cn</email></author><entry><title type="html">RoboBrain 2.0: Embodied Intelligence with Multimodal Models</title><link href="https://ruihanwu.github.io/robobrain/" rel="alternate" type="text/html" title="RoboBrain 2.0: Embodied Intelligence with Multimodal Models" /><published>2025-10-26T00:00:00+00:00</published><updated>2025-10-26T00:00:00+00:00</updated><id>https://ruihanwu.github.io/robobrain</id><content type="html" xml:base="https://ruihanwu.github.io/robobrain/"><![CDATA[<p>RoboBrain 2.0 integrates visual-language-action reasoning through multimodal world models.<br />
It enables agents to learn interactive tasks grounded in perception and embodiment…</p>]]></content><author><name>Ruihan Wu</name><email>22307140084@m.fudan.edu.cn</email></author><summary type="html"><![CDATA[RoboBrain 2.0 integrates visual-language-action reasoning through multimodal world models. It enables agents to learn interactive tasks grounded in perception and embodiment…]]></summary></entry></feed>